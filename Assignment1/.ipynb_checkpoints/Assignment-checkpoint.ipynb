{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bd568d-3568-4a08-914a-1d671e6055da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment1: Prototypical Networks\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/oscarknagg/few-shot/master/assets/proto_nets_diagram.png\"/></center>\n",
    "\n",
    "> Figure 1: Prototypical networks in a nutshell. In a 3-way 5-shot classification task, the class\n",
    "prototypes c1, c2, c3 are computed from each class’s support features (colored circles). The\n",
    "prototypes define decision boundaries based on Euclidean distance. A query example $x$\n",
    "is determined to be class 2 since its features (white circle) lie within that class’s decision\n",
    "region.\n",
    "\n",
    "\n",
    "In this assignment, let's use [**prototypical networks**](https://papers.nips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) to solve the *n-way k-shot few-shot learning* problem with the [Omniglot](https://www.tensorflow.org/datasets/catalog/omniglot) dataset. \n",
    "\n",
    "- **IMPORTANT**: 과제 제출하실 때 레포트를 pdf로 변환하고, `Assignment.ipynb`, `dataloader.py`, `model.py`, `train.py`, 모두 압축해서 **이름_학번_Assignment1.zip** 형식으로 ETL에 올리시길 바랍니다.\n",
    "- Assignment 1 is due 23:59 PM 4/14. Note that 10%p of your score will be deducted for every hour you are late.\n",
    "- Make sure you are using the latest version of [TensorFlow](https://www.tensorflow.org/api_docs/python/tf), or at least the 2.3 version.\n",
    "- *If you have any questions regarding the assignment, please contact me at steve2972@snu.ac.kr*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd832e9-f3f6-475f-9349-3caef2d1c554",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "We define a **task** to be an *n-way k-shot* image classification problem. The maximum number of tasks that can be created from a dataset can be defined as $M \\choose N$$\\cdot$$ K \\choose k$ where $M$=the number of classes in the dataset, $N$=the number of classes in a task, $K$=the total number of images in a class, and $k$=the number of shots in a class. In this problem, we define a method to create an upper bound to the number of tasks used during training. \n",
    "\n",
    "**Problem**: In the `dataloader.py` file, complete the implementation of the `DataLoader.generate_task_list` method and the `DataLoader.data_generator` method. Details concerning the implementation are as follows:\n",
    "\n",
    "- The `generate_task_list` method generates a **list of dictionaries** where each dictionary comprises a **task** in the form of `{class_name: [random sequence of image indexes]}`\n",
    "    - For example, a 2-way 5-shot task for both the support & query datasets can be generated as `{'A': [1,3,5,7,9,2,4,6,8, 10], 'B': [8,6,4,10,2,1,5,3,7,9]}`\n",
    "    - Note that the length of the random sequence of image indexes should equal the # of shots in the support dataset $+$ # of shots in the query dataset.\n",
    "    \n",
    "- The `data_generator` method generates a **support** and a **query dataset** where each dataset is in the form of a numpy array with shape `[num_way, num_shot, image_width, image_height, num_channels]`.\n",
    "    - Note that `data_generator` is created based on tasks which have the form `{class_name: [random sequence of image indexes]}`\n",
    "    \n",
    "If the `dataloader.py` file is implemented correctly, the following code should run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2382c2-8f38-4179-a7bb-1387c551bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fembem\\anaconda3\\envs\\tf-2.8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train Omniglot dataset\n",
      "Finished preprocessing\n",
      "Preprocessing test Omniglot dataset\n",
      "Finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "num_ways = 5\n",
    "support_shots = 5\n",
    "query_shots = 5\n",
    "\n",
    "train_dataset = DataLoader('train', n_way=num_ways, n_support=support_shots, n_query=query_shots)\n",
    "val_dataset = DataLoader('test', n_way=num_ways, n_support=support_shots, n_query=query_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bff77-39a0-4950-8305-a964d802b361",
   "metadata": {},
   "source": [
    "**If implemented correctly, we can also visualize a random task using the `visualize_random_task` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e14ae5c-8949-4afb-90d7-99505264df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.data.keys()\n",
    "train_dataset.generate_task_list(n_tasks=1)\n",
    "# train_dataset.task_list\n",
    "# train_dataset.visualize_random_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3d864-c566-4f3f-b607-f10668d246df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Next we will implement the prototypical network and examine how the performance of the model changes when **the total number of tasks is changed from 1k to 10k**.\n",
    "\n",
    "As discussed in the lecture, the basic idea of protonets is to learn a mapping $f_\\theta(\\cdot)$ from images to features such that images of the same class are close to each other in a feature space. Central to this is the notion of a *prototype*\n",
    "\n",
    "$$c_n = \\frac{1}{K} \\sum_{(x,y)\\in\\mathcal{D}^{tr}_i:y=n} f_\\theta(x)$$\n",
    "\n",
    "i.e. for task $i$, the prototype of the $n$-th class $c_n$ is defined as the mean of the $K$ feature\n",
    "vectors of that class’s support images. To classify some image $x$, we compute a measure of\n",
    "distance $d$ between $f_\\theta(x)$ and each of the prototypes. We will use the squared Euclidean\n",
    "distance:\n",
    "\n",
    "$$d(f_\\theta(x),c_n) = ||f_\\theta(x) - c_n ||^2_2$$\n",
    "\n",
    "We interpret the negative squared distances as **logits**, or *unnormalized log-probabilities,\n",
    "of* $x$ *belonging to each class*. To obtain the proper probabilities, we apply the softmax\n",
    "operation:\n",
    "\n",
    "$$p_\\theta(y = n|x) = \\frac{\\exp(-d(f_\\theta(x),c_n))}{\\sum^N_{n'=1}\\exp(-d(f_\\theta(x),c_{n'}))}$$\n",
    "\n",
    "Because the softmax operation preserves ordering, the class whose prototype is closest to\n",
    "$f_\\theta(x)$ is naturally interpreted as the most likely class for $x$. To train the model to generalize,\n",
    "we compute prototypes using support data, but minimize the negative log likelihood of the query data\n",
    "\n",
    "$$\\mathcal{J}(\\theta) = \\mathbb{E}_{\\mathcal{T}\\sim p(\\mathcal{T}), (\\mathcal{D}^{tr}, \\mathcal{D}^{ts})\\sim \\mathcal{T}} \\left[ \\frac{1}{NQ} \\sum_{(x^{ts},y^{ts})\\sim\\mathcal{D}^{ts}} -\\log p_\\theta(y=y^{ts} | x^{ts})\\right]$$\n",
    "\n",
    "Notice that this is equivalent to using a cross-entropy loss.\n",
    "\n",
    "We optimize $\\theta$ using Adam, and as is standard for stochastic gradient methods, we approximate the objective for $\\mathcal{J}$ with\n",
    "Monte Carlo estimation on minibatches of tasks. Thus, for one minibatch of size $B$, we have\n",
    "\n",
    "$$\\mathcal{J}(\\theta) \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ \\frac{1}{NQ} \\sum_{(x^{ts},y^{ts})\\sim\\mathcal{D}^{ts}} -\\log p_\\theta(y=y^{ts} | x^{ts})\\right]$$\n",
    "\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "1. In the `model.py` file, complete the implementation of the `Prototypical_Network.call` method. Pay attention to the inline comments.\n",
    "2. Test the model with different amounts of training tasks (e.g. from 1k to 10k).\n",
    "    1.  Create a plot of the validation accuracy using `matplotlib` and report your findings. \n",
    "        - Train and validate the model on 5-way 5-shot tasks for both the support and query datasets.\n",
    "        - [Hint]: you should obtain a query accuracy on the validation split of **at least** 97% with 10k tasks.\n",
    "    2. Also plot how the degree of overfitting (the difference between training accuracy and validation accuracy) changes with different amounts of tasks.\n",
    "3. Repeat (2) with different tasks [5way-1shot, 20way-5shot, 20way-1shot] and report your findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf817cd-de2e-463c-9016-64cee359d2b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "num_tasks = 10000 # The total number of tasks in the predefined task distribution\n",
    "num_epochs = 100 # The number of epochs to train the model\n",
    "num_tasks_per_epoch = 100 # The number of tasks to be trained on for each epoch\n",
    "\n",
    "# metrics = [train accuracies, train losses, validation accuracies, validation losses]\n",
    "metrics = train.train_model(train_dataset, val_dataset, \n",
    "                            n_tasks=num_tasks, n_epochs=num_epochs, \n",
    "                            n_tpe=num_tasks_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a6395-a092-440c-92be-d0bf79a0db74",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Finally, we will evaluate the generalization performance of the model by using a **different test setting from a training setting**.\n",
    "\n",
    "<br>\n",
    "\n",
    "As prototypical networks are based on a nonparametric method, we can flexibly train the model with different types of tasks (e.g. 5way1shot tasks + 5way5shot tasks). Let's assume we are allowed to use up to 30 training samples to train a single task. Evaluate the performance of the model when we train the model using **randomly defined training tasks**.\n",
    "\n",
    "\n",
    "\n",
    "**Problem**:\n",
    "\n",
    "1. In the `dataloader.py` file, implement the `random_data_generator` method.\n",
    "2. Using the experimental details outlined below, run experiments and compare with the results from Problem 2\n",
    "    - Note that you do not need to specify a set number of tasks for this problem\n",
    "\n",
    "\n",
    "**Experimental Details**:\n",
    "- *Train setting*: Assume to use exactly 30 images for each task (for support + query). First randomly select $N$ from the set $\\{2, 3, 5, 6, 10, 15\\}$. This means that each $N$ can use $\\{15, 10, 6, 5, 3, 2\\}$ samples per class respectively. By properly splitting the number of samples into 2 splits (support/query), we can randomly define different tasks.\n",
    "    - For example, an example support/query task can be defined as support={5-way 4-shot}, and query={5-way 2-shot}.\n",
    "    - *Extra Credit (optional)*: implement and test a method where we use a flexible number of images (not limited to just 30) where $N$ can also be defined randomly.\n",
    "- *Test setting*: 5way 5 shot (5$\\cdot$5 for support + 5$\\cdot$1 for query = 30 images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b81f55-0b23-41b0-9a2c-44164eb58f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ways = 5\n",
    "support_shots = 5\n",
    "query_shots = 1\n",
    "\n",
    "train_dataset = DataLoader('train')\n",
    "val_dataset = DataLoader('test', n_way=num_ways, n_support=support_shots, n_query=query_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d925446-05fd-4ea0-9422-aab4c82e0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # The number of epochs to train the model\n",
    "num_tasks_per_epoch = 100 # The number of tasks to be trained on for each epoch\n",
    "\n",
    "# metrics = [train accuracies, train losses, validation accuracies, validation losses]\n",
    "metrics = train.train_model(train_dataset, val_dataset, \n",
    "                            n_tasks=num_tasks, n_epochs=num_epochs, \n",
    "                            n_tpe=num_tasks_per_epoch, is_random=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
